diff --git a/codex-rs/core/src/codex.rs b/codex-rs/core/src/codex.rs
index fb2a3b058..4e5025063 100644
--- a/codex-rs/core/src/codex.rs
+++ b/codex-rs/core/src/codex.rs
@@ -1,4 +1,5 @@
 use std::collections::HashMap;
+use std::collections::HashSet;
 use std::fmt::Debug;
 use std::path::PathBuf;
 use std::sync::Arc;
@@ -64,6 +65,7 @@ use tokio::sync::Mutex;
 use tokio::sync::RwLock;
 use tokio::sync::oneshot;
 use tokio_util::sync::CancellationToken;
+use toml_edit::value;
 use tracing::Instrument;
 use tracing::debug;
 use tracing::error;
@@ -83,6 +85,8 @@ use crate::config::Config;
 use crate::config::Constrained;
 use crate::config::ConstraintResult;
 use crate::config::GhostSnapshotConfig;
+use crate::config::edit::ConfigEdit;
+use crate::config::edit::ConfigEditsBuilder;
 use crate::config::types::ShellEnvironmentPolicy;
 use crate::context_manager::ContextManager;
 use crate::environment_context::EnvironmentContext;
@@ -257,6 +261,7 @@ impl Codex {
         }
         let model = models_manager.get_model(&config.model, &config).await;
         let session_configuration = SessionConfiguration {
+            provider_id: config.model_provider_id.clone(),
             provider: config.model_provider.clone(),
             model: model.clone(),
             model_reasoning_effort: config.model_reasoning_effort,
@@ -406,6 +411,9 @@ impl TurnContext {
 #[derive(Clone)]
 pub(crate) struct SessionConfiguration {
     /// Provider identifier ("openai", "openrouter", ...).
+    provider_id: String,
+
+    /// Provider configuration.
     provider: ModelProviderInfo,
 
     /// If not specified, server will use its default model.
@@ -467,6 +475,17 @@ impl SessionConfiguration {
         if let Some(cwd) = updates.cwd.clone() {
             next_configuration.cwd = cwd;
         }
+        if let (Some(provider_id), Some(provider)) = (
+            updates.model_provider_id.clone(),
+            updates.model_provider.clone(),
+        ) {
+            next_configuration.provider_id = provider_id;
+            next_configuration.provider = provider;
+            let mut updated_config = (*next_configuration.original_config_do_not_use).clone();
+            updated_config.model_provider_id = next_configuration.provider_id.clone();
+            updated_config.model_provider = next_configuration.provider.clone();
+            next_configuration.original_config_do_not_use = Arc::new(updated_config);
+        }
         Ok(next_configuration)
     }
 }
@@ -477,6 +496,8 @@ pub(crate) struct SessionSettingsUpdate {
     pub(crate) approval_policy: Option<AskForApproval>,
     pub(crate) sandbox_policy: Option<SandboxPolicy>,
     pub(crate) model: Option<String>,
+    pub(crate) model_provider_id: Option<String>,
+    pub(crate) model_provider: Option<ModelProviderInfo>,
     pub(crate) reasoning_effort: Option<Option<ReasoningEffortConfig>>,
     pub(crate) reasoning_summary: Option<ReasoningSummaryConfig>,
     pub(crate) final_output_json_schema: Option<Option<Value>>,
@@ -1812,6 +1833,7 @@ mod handlers {
                     reasoning_effort: Some(effort),
                     reasoning_summary: Some(summary),
                     final_output_json_schema: Some(final_output_json_schema),
+                    ..Default::default()
                 },
             ),
             Op::UserInput {
@@ -2307,6 +2329,7 @@ pub(crate) async fn run_task(
         return None;
     }
 
+    let mut turn_context = turn_context;
     let model_info = turn_context.client.get_model_info();
     let auto_compact_limit = model_info.auto_compact_token_limit().unwrap_or(i64::MAX);
     let total_usage_tokens = sess.get_total_token_usage().await;
@@ -2387,7 +2410,12 @@ pub(crate) async fn run_task(
         )
         .await
         {
-            Ok(turn_output) => {
+            Ok(outcome) => {
+                let TurnRunOutcome {
+                    result: turn_output,
+                    turn_context: updated_context,
+                } = outcome;
+                turn_context = updated_context;
                 let TurnRunResult {
                     needs_follow_up,
                     last_agent_message: turn_last_agent_message,
@@ -2447,6 +2475,142 @@ async fn run_auto_compact(sess: &Arc<Session>, turn_context: &Arc<TurnContext>)
     }
 }
 
+fn should_switch_model_provider(err: &CodexErr, retries: u64, max_retries: u64) -> bool {
+    if matches!(err, CodexErr::EnvVar(_) | CodexErr::RetryLimit(_)) {
+        return true;
+    }
+    if let Some(status) = err.http_status_code_value()
+        && matches!(status, 401 | 403 | 429)
+    {
+        return true;
+    }
+    err.is_retryable() && retries >= max_retries
+}
+
+fn next_provider_from_pool(
+    config: &Config,
+    current_provider_id: &str,
+    attempted_provider_ids: &mut HashSet<String>,
+) -> Option<(String, ModelProviderInfo)> {
+    let pool = &config.model_provider_pool;
+    let pool_len = pool.len();
+    if pool_len == 0 {
+        return None;
+    }
+
+    let start_index = pool
+        .iter()
+        .position(|provider_id| provider_id == current_provider_id)
+        .map(|index| (index + 1) % pool_len)
+        .unwrap_or(0);
+
+    for offset in 0..pool_len {
+        let index = (start_index + offset) % pool_len;
+        let provider_id = &pool[index];
+        if provider_id == current_provider_id || attempted_provider_ids.contains(provider_id) {
+            continue;
+        }
+        let Some(provider) = config.model_providers.get(provider_id) else {
+            warn!("model_provider_pool entry `{provider_id}` not found in model_providers");
+            attempted_provider_ids.insert(provider_id.clone());
+            continue;
+        };
+        if let Err(err) = provider.api_key() {
+            warn!("Skipping model provider {provider_id}: {err}");
+            attempted_provider_ids.insert(provider_id.clone());
+            continue;
+        }
+        attempted_provider_ids.insert(provider_id.clone());
+        return Some((provider_id.clone(), provider.clone()));
+    }
+
+    None
+}
+
+async fn persist_model_provider_selection(config: &Config, provider_id: &str) {
+    let segments = if let Some(profile) = config.active_profile.as_deref() {
+        vec![
+            "profiles".to_string(),
+            profile.to_string(),
+            "model_provider".to_string(),
+        ]
+    } else {
+        vec!["model_provider".to_string()]
+    };
+    let edits = vec![ConfigEdit::SetPath {
+        segments,
+        value: value(provider_id.to_string()),
+    }];
+
+    if let Err(err) = ConfigEditsBuilder::new(&config.codex_home)
+        .with_edits(edits)
+        .apply()
+        .await
+    {
+        warn!("failed to persist model provider switch to {provider_id}: {err}");
+    }
+}
+
+async fn maybe_switch_model_provider(
+    sess: &Arc<Session>,
+    turn_context: &Arc<TurnContext>,
+    attempted_provider_ids: &mut HashSet<String>,
+    err: &CodexErr,
+    retries: u64,
+    max_retries: u64,
+) -> Option<Arc<TurnContext>> {
+    if !should_switch_model_provider(err, retries, max_retries) {
+        return None;
+    }
+
+    let config = turn_context.client.config();
+    let current_provider_id = {
+        let state = sess.state.lock().await;
+        state.session_configuration.provider_id.clone()
+    };
+    let (next_provider_id, next_provider) = next_provider_from_pool(
+        config.as_ref(),
+        current_provider_id.as_str(),
+        attempted_provider_ids,
+    )?;
+
+    if let Err(err) = sess
+        .update_settings(SessionSettingsUpdate {
+            model_provider_id: Some(next_provider_id.clone()),
+            model_provider: Some(next_provider.clone()),
+            ..Default::default()
+        })
+        .await
+    {
+        warn!("failed to switch model provider to {next_provider_id}: {err}");
+        return None;
+    }
+
+    let session_configuration = {
+        let state = sess.state.lock().await;
+        state.session_configuration.clone()
+    };
+    let updated_context = sess
+        .new_turn_from_configuration(
+            turn_context.sub_id.clone(),
+            session_configuration,
+            Some(turn_context.final_output_json_schema.clone()),
+            false,
+        )
+        .await;
+
+    persist_model_provider_selection(config.as_ref(), next_provider_id.as_str()).await;
+    sess.notify_background_event(
+        updated_context.as_ref(),
+        format!(
+            "Switched model provider to {next_provider_id} (from {current_provider_id}) after {err}"
+        ),
+    )
+    .await;
+
+    Some(updated_context)
+}
+
 #[instrument(level = "trace",
     skip_all,
     fields(
@@ -2457,11 +2621,11 @@ async fn run_auto_compact(sess: &Arc<Session>, turn_context: &Arc<TurnContext>)
 )]
 async fn run_turn(
     sess: Arc<Session>,
-    turn_context: Arc<TurnContext>,
+    mut turn_context: Arc<TurnContext>,
     turn_diff_tracker: SharedTurnDiffTracker,
     input: Vec<ResponseItem>,
     cancellation_token: CancellationToken,
-) -> CodexResult<TurnRunResult> {
+) -> CodexResult<TurnRunOutcome> {
     let mcp_tools = sess
         .services
         .mcp_connection_manager
@@ -2494,6 +2658,12 @@ async fn run_turn(
     };
 
     let mut retries = 0;
+    let mut attempted_provider_ids = {
+        let mut attempted = HashSet::new();
+        let state = sess.state.lock().await;
+        attempted.insert(state.session_configuration.provider_id.clone());
+        attempted
+    };
     loop {
         let err = match try_run_turn(
             Arc::clone(&router),
@@ -2505,7 +2675,12 @@ async fn run_turn(
         )
         .await
         {
-            Ok(output) => return Ok(output),
+            Ok(output) => {
+                return Ok(TurnRunOutcome {
+                    turn_context,
+                    result: output,
+                });
+            }
             Err(CodexErr::ContextWindowExceeded) => {
                 sess.set_total_tokens_full(&turn_context).await;
                 return Err(CodexErr::ContextWindowExceeded);
@@ -2520,12 +2695,27 @@ async fn run_turn(
             Err(err) => err,
         };
 
+        // Use the configured provider-specific stream retry budget.
+        let max_retries = turn_context.client.get_provider().stream_max_retries();
+        if let Some(updated_context) = maybe_switch_model_provider(
+            &sess,
+            &turn_context,
+            &mut attempted_provider_ids,
+            &err,
+            retries,
+            max_retries,
+        )
+        .await
+        {
+            turn_context = updated_context;
+            retries = 0;
+            continue;
+        }
+
         if !err.is_retryable() {
             return Err(err);
         }
 
-        // Use the configured provider-specific stream retry budget.
-        let max_retries = turn_context.client.get_provider().stream_max_retries();
         if retries < max_retries {
             retries += 1;
             let delay = match &err {
@@ -2553,6 +2743,12 @@ async fn run_turn(
     }
 }
 
+#[derive(Debug)]
+struct TurnRunOutcome {
+    turn_context: Arc<TurnContext>,
+    result: TurnRunResult,
+}
+
 #[derive(Debug)]
 struct TurnRunResult {
     needs_follow_up: bool,
@@ -3133,6 +3329,7 @@ mod tests {
         let config = Arc::new(config);
         let model = ModelsManager::get_model_offline(config.model.as_deref());
         let session_configuration = SessionConfiguration {
+            provider_id: config.model_provider_id.clone(),
             provider: config.model_provider.clone(),
             model,
             model_reasoning_effort: config.model_reasoning_effort,
@@ -3199,6 +3396,7 @@ mod tests {
         let config = Arc::new(config);
         let model = ModelsManager::get_model_offline(config.model.as_deref());
         let session_configuration = SessionConfiguration {
+            provider_id: config.model_provider_id.clone(),
             provider: config.model_provider.clone(),
             model,
             model_reasoning_effort: config.model_reasoning_effort,
@@ -3449,6 +3647,7 @@ mod tests {
         let agent_status = Arc::new(RwLock::new(AgentStatus::PendingInit));
         let model = ModelsManager::get_model_offline(config.model.as_deref());
         let session_configuration = SessionConfiguration {
+            provider_id: config.model_provider_id.clone(),
             provider: config.model_provider.clone(),
             model,
             model_reasoning_effort: config.model_reasoning_effort,
@@ -3543,6 +3742,7 @@ mod tests {
         let agent_status = Arc::new(RwLock::new(AgentStatus::PendingInit));
         let model = ModelsManager::get_model_offline(config.model.as_deref());
         let session_configuration = SessionConfiguration {
+            provider_id: config.model_provider_id.clone(),
             provider: config.model_provider.clone(),
             model,
             model_reasoning_effort: config.model_reasoning_effort,
diff --git a/codex-rs/core/src/config/mod.rs b/codex-rs/core/src/config/mod.rs
index 6099e1c29..11049c3e5 100644
--- a/codex-rs/core/src/config/mod.rs
+++ b/codex-rs/core/src/config/mod.rs
@@ -116,6 +116,9 @@ pub struct Config {
     /// Info needed to make an API request to the model.
     pub model_provider: ModelProviderInfo,
 
+    /// Ordered list of provider IDs to use for automatic failover.
+    pub model_provider_pool: Vec<String>,
+
     /// Approval policy for executing commands.
     pub approval_policy: Constrained<AskForApproval>,
 
@@ -679,6 +682,10 @@ pub struct ConfigToml {
     /// Provider to use from the model_providers map.
     pub model_provider: Option<String>,
 
+    /// Ordered list of provider IDs to use for automatic failover.
+    #[serde(default)]
+    pub model_provider_pool: Vec<String>,
+
     /// Size of the context window for the model, in tokens.
     pub model_context_window: Option<i64>,
 
@@ -1204,6 +1211,30 @@ impl Config {
             model_providers.entry(key).or_insert(provider);
         }
 
+        let raw_model_provider_pool = config_profile
+            .model_provider_pool
+            .clone()
+            .unwrap_or(cfg.model_provider_pool);
+        let model_provider_pool = {
+            let mut seen = std::collections::HashSet::new();
+            raw_model_provider_pool
+                .into_iter()
+                .filter_map(|provider| {
+                    let trimmed = provider.trim();
+                    if trimmed.is_empty() {
+                        None
+                    } else {
+                        let trimmed = trimmed.to_string();
+                        if seen.insert(trimmed.clone()) {
+                            Some(trimmed)
+                        } else {
+                            None
+                        }
+                    }
+                })
+                .collect::<Vec<_>>()
+        };
+
         let model_provider_id = model_provider
             .or(config_profile.model_provider)
             .or(cfg.model_provider)
@@ -1326,6 +1357,7 @@ impl Config {
             model_auto_compact_token_limit: cfg.model_auto_compact_token_limit,
             model_provider_id,
             model_provider,
+            model_provider_pool,
             cwd: resolved_cwd,
             approval_policy: constrained_approval_policy,
             sandbox_policy: constrained_sandbox_policy,
@@ -1903,6 +1935,44 @@ trust_level = "trusted"
         Ok(())
     }
 
+    #[test]
+    fn profile_model_provider_pool_overrides_base() -> std::io::Result<()> {
+        let codex_home = TempDir::new()?;
+        let mut profiles = HashMap::new();
+        profiles.insert(
+            "work".to_string(),
+            ConfigProfile {
+                model_provider_pool: Some(vec![
+                    "profile-primary".to_string(),
+                    "profile-secondary".to_string(),
+                ]),
+                ..Default::default()
+            },
+        );
+        let cfg = ConfigToml {
+            model_provider_pool: vec!["base-provider".to_string()],
+            profiles,
+            profile: Some("work".to_string()),
+            ..Default::default()
+        };
+
+        let config = Config::load_from_base_config_with_overrides(
+            cfg,
+            ConfigOverrides::default(),
+            codex_home.path().to_path_buf(),
+        )?;
+
+        assert_eq!(
+            config.model_provider_pool,
+            vec![
+                "profile-primary".to_string(),
+                "profile-secondary".to_string()
+            ]
+        );
+
+        Ok(())
+    }
+
     #[test]
     fn profile_sandbox_mode_overrides_base() -> std::io::Result<()> {
         let codex_home = TempDir::new()?;
@@ -3177,6 +3247,7 @@ model_verbosity = "high"
                 model_auto_compact_token_limit: None,
                 model_provider_id: "openai".to_string(),
                 model_provider: fixture.openai_provider.clone(),
+                model_provider_pool: Vec::new(),
                 approval_policy: Constrained::allow_any(AskForApproval::Never),
                 sandbox_policy: Constrained::allow_any(SandboxPolicy::new_read_only_policy()),
                 did_user_set_custom_approval_policy_or_sandbox_mode: true,
@@ -3261,6 +3332,7 @@ model_verbosity = "high"
             model_auto_compact_token_limit: None,
             model_provider_id: "openai-chat-completions".to_string(),
             model_provider: fixture.openai_chat_completions_provider.clone(),
+            model_provider_pool: Vec::new(),
             approval_policy: Constrained::allow_any(AskForApproval::UnlessTrusted),
             sandbox_policy: Constrained::allow_any(SandboxPolicy::new_read_only_policy()),
             did_user_set_custom_approval_policy_or_sandbox_mode: true,
@@ -3360,6 +3432,7 @@ model_verbosity = "high"
             model_auto_compact_token_limit: None,
             model_provider_id: "openai".to_string(),
             model_provider: fixture.openai_provider.clone(),
+            model_provider_pool: Vec::new(),
             approval_policy: Constrained::allow_any(AskForApproval::OnFailure),
             sandbox_policy: Constrained::allow_any(SandboxPolicy::new_read_only_policy()),
             did_user_set_custom_approval_policy_or_sandbox_mode: true,
@@ -3445,6 +3518,7 @@ model_verbosity = "high"
             model_auto_compact_token_limit: None,
             model_provider_id: "openai".to_string(),
             model_provider: fixture.openai_provider.clone(),
+            model_provider_pool: Vec::new(),
             approval_policy: Constrained::allow_any(AskForApproval::OnFailure),
             sandbox_policy: Constrained::allow_any(SandboxPolicy::new_read_only_policy()),
             did_user_set_custom_approval_policy_or_sandbox_mode: true,
diff --git a/codex-rs/core/src/config/profile.rs b/codex-rs/core/src/config/profile.rs
index e1c45c1f1..b632182d4 100644
--- a/codex-rs/core/src/config/profile.rs
+++ b/codex-rs/core/src/config/profile.rs
@@ -16,6 +16,8 @@ pub struct ConfigProfile {
     /// The key in the `model_providers` map identifying the
     /// [`ModelProviderInfo`] to use.
     pub model_provider: Option<String>,
+    /// Ordered list of provider IDs to use for automatic failover.
+    pub model_provider_pool: Option<Vec<String>>,
     pub approval_policy: Option<AskForApproval>,
     pub sandbox_mode: Option<SandboxMode>,
     pub model_reasoning_effort: Option<ReasoningEffort>,
diff --git a/docs/config.md b/docs/config.md
index 2b64253d3..e9a5532c7 100644
--- a/docs/config.md
+++ b/docs/config.md
@@ -6,6 +6,25 @@ For advanced configuration instructions, see [this documentation](https://develo
 
 For a full configuration reference, see [this documentation](https://developers.openai.com/codex/config-reference).
 
+## Model provider pool
+
+Codex can automatically fail over to a prioritized list of providers when the
+current provider fails. Configure `model_provider_pool` at the top level or per
+profile in `~/.codex/config.toml`:
+
+```toml
+model_provider = "openai"
+model_provider_pool = ["ppai", "vector", "openai"]
+
+[profiles.gemini]
+model_provider = "gemini"
+model_provider_pool = ["gemini", "openai"]
+```
+
+When a switch happens, Codex updates `model_provider` in the active profile (if
+one is set) or at the top level of `config.toml`. The pool is tried in order and
+wraps back to the start until each entry has been attempted once per turn.
+
 ## Connecting to MCP servers
 
 Codex can connect to MCP servers configured in `~/.codex/config.toml`. See the configuration reference for the latest MCP server options:
